{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二章 感知机\n",
    "- 判别模型\n",
    "- 优点是易于实现\n",
    "- 二分类问题\n",
    "- 线性分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 模型\n",
    "\n",
    "$f(x)=sign(w \\cdot x + b)$\n",
    "\n",
    "其中 $w \\in R^n , b \\in R$ w 叫做权值向量(weight)，b叫做偏置(bias);sign是符号函数。\n",
    "\n",
    "模型的假设空间为：\n",
    "${f|f(x) = w \\cdot x + b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron():\n",
    "    def __init__(self,dim = 1):\n",
    "        self.dim = dim\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self._init_params()\n",
    "        \n",
    "    def _init_params(self):\n",
    "        self.w = np.random.normal(size = (1,self.dim))\n",
    "        self.b = np.random.normal()\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return np.sign(np.dot(self.w,x.T) + self.b).reshape(-1,1)\n",
    "                       \n",
    "test_model = Perceptron(5)\n",
    "xs = np.array([[1,2,3,4,5]])\n",
    "test_model(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几何意义\n",
    "\n",
    "对于特征空间 $R^n$ 中的一个超平面， $w$ 是超平面的法向量， b是超平面的截距。，位于超平面两边的点被分为两类，因此叫做分离超平面(separating hyperplane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xion/anaconda3/lib/python3.7/site-packages/matplotlib/figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'matplotlib.pyplot' from '/home/xion/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def line(x):\n",
    "    return np.sign(np.dot([1,1],x.T)-9.5)\n",
    "\n",
    "random_data = np.random.randint(0,10,size=(30,2))\n",
    "label = line(random_data)\n",
    "for d,l in zip(random_data,label):\n",
    "    if l > 0:\n",
    "        makers = 'o'\n",
    "    else:\n",
    "        makers = 'x'\n",
    "        \n",
    "    plt.scatter(d[0],d[1],color = 'b',marker=makers)\n",
    "plt.plot([9.5,0],[0,9.5])\n",
    "plt.plot([3.5,6],[5.5,8])\n",
    "plt.annotate('[1,1]',[5.5+0.5,8-0.5])\n",
    "plt.annotate('w.x+b',[9.5-1.5,1.5])\n",
    "plt.axes().set_aspect('equal')\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 学习策略\n",
    "\n",
    "## 2.2.1 线性可分数据集\n",
    "给定一个数据集\n",
    "\n",
    "$T =\\{ \\{x_1,y_1\\},\\{x_2,y_2\\},\\dots\\{x_n,y_n\\}\\}$\n",
    "\n",
    "如果存在一个超平面S $w \\cdot x +b = 0$ 能够将数据集合所有的正实例和负实例完全分到平面两侧，那么这个数据集就叫做线性可分的，否则，称数据集T为线性不可分。\n",
    "\n",
    "## 2.2.2 感知机学习策略(loss)\n",
    "\n",
    "损失函数的自然选择是误分类点的数量，但是这样的损失函数是一个离散值。不易优化，所以选择一个连续值，所有误分类点到超平面的距离之和。\n",
    "\n",
    "\n",
    "点到平面的距离公式为：\n",
    "\n",
    "$\\frac 1 {||w||} |w \\cdot x_0 +b|$ （1）\n",
    "\n",
    "$||w||$ 是 w 的 l2 范数\n",
    "\n",
    "对于误分类的点来说有\n",
    "\n",
    "$-y_i (w \\cdot x +b) > 0 $ (2)\n",
    "\n",
    "结合(1)和（2）, 假设误分类的点的集合为 M，那么可以得到损失函数为：\n",
    "\n",
    "$- \\frac 1 {||w||} \\sum_{x_i \\in M} y_i (w \\cdot x_i + b)$\n",
    "\n",
    "\n",
    "这里的||w||可以忽略，整理得到\n",
    "\n",
    "$L(w,b) = - \\sum_{x_i \\in M} y_i (w \\cdot x_i + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss of true model is 0\n",
      "the loss of a new model is -458.476085759703\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PerceptronLoss():\n",
    "    def __init__(self,model):\n",
    "        self.w = model.w \n",
    "        self.b = model.b \n",
    "        self.model = model\n",
    "        \n",
    "    \n",
    "    def __call__(self,xs,labels):\n",
    "        return self.loss(xs,labels) \n",
    "    \n",
    "    def loss(self,xs,labels):\n",
    "        predict = self.model(xs)\n",
    "        if (predict==labels).all():\n",
    "            return 0\n",
    "        need_index = (predict!=labels).reshape(-1)\n",
    "        \n",
    "        wrong_sample = predict[need_index]\n",
    "        wrong_xs = xs[need_index,:]\n",
    "        \n",
    "        return np.sum(wrong_sample * np.dot(self.w,wrong_xs.T) + self.b)\n",
    "            \n",
    "# 假设数据的分布是从这样一个线性模型中产生的       \n",
    "true_model = Perceptron(5)\n",
    "true_model.w = np.array([1,10,-10,10,-10])\n",
    "true_model.b = np.array(1)\n",
    "\n",
    "data = np.random.randint(-20,20,size=(50,5))\n",
    "labels = true_model(data)\n",
    "\n",
    "\n",
    "true_model_loss = PerceptronLoss(true_model)\n",
    "# 因为符合模型分布，所以损失为0\n",
    "print('the loss of true model is %s' % true_model_loss(data,labels))\n",
    "\n",
    "# 如果使用新的还没训练的模型，那么损失就不是0\n",
    "loss = PerceptronLoss(test_model)\n",
    "print('the loss of a new model is %s' % loss(data,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 感知机学习算法\n",
    "\n",
    "我们要求解的问题为：\n",
    "\n",
    "$min_{w,b}L(w,b)= - \\sum_{x_i \\in M} y_i(w \\cdot x_i +b)$\n",
    "\n",
    "### 梯度下降法\n",
    "\n",
    "感知机学习的算法叫做随机梯度下降法。\n",
    "\n",
    "损失函数的在w,和b上的梯度分别为：\n",
    "\n",
    "$\\nabla _w L(w,b) = - \\sum_{x_i \\in M}y_i x_i$\n",
    "\n",
    "$\\nabla _b L(w,b) = - \\sum_{x_i \\in M}y_i  $\n",
    "\n",
    "随机选取一个误分类点，(x_i,y_i),对 w 和 b 进行更新\n",
    "\n",
    "$w \\gets w + \\eta y_i x_i$\n",
    "\n",
    "\n",
    "$b \\gets b + \\eta y_i $\n",
    "直到 loss 为 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True w:[  1  10 -10  10 -10] True b:1\n",
      "test_mode w:[[-1.    1.79 -1.17 -0.23 -0.71]] b:[-1.1]\n",
      "test_mode w:[[-0.73  1.77 -0.94  0.09 -0.86]] b:[-1.1]\n",
      "test_mode w:[[-0.65  1.7  -0.94  0.03 -0.99]] b:[-1.11]\n",
      "test_mode w:[[-0.36  1.75 -0.85  0.16 -1.07]] b:[-1.11]\n",
      "test_mode w:[[-0.47  1.56 -0.83  0.36 -1.23]] b:[-1.12]\n",
      "[Epoch:0]model loss: -16.06336369068795\n",
      "test_mode w:[[-0.47  1.56 -0.83  0.36 -1.23]] b:[-1.12]\n",
      "test_mode w:[[-0.31  1.51 -0.72  0.54 -1.24]] b:[-1.11]\n",
      "test_mode w:[[-0.31  1.51 -0.72  0.54 -1.24]] b:[-1.11]\n",
      "test_mode w:[[-0.31  1.51 -0.72  0.54 -1.24]] b:[-1.11]\n",
      "test_mode w:[[-0.31  1.51 -0.72  0.54 -1.24]] b:[-1.11]\n",
      "[Epoch:1]model loss: -16.06336369068795\n",
      "test_mode w:[[-0.31  1.51 -0.72  0.54 -1.24]] b:[-1.11]\n",
      "test_mode w:[[-0.15  1.46 -0.61  0.72 -1.25]] b:[-1.1]\n",
      "test_mode w:[[-0.15  1.46 -0.61  0.72 -1.25]] b:[-1.1]\n",
      "test_mode w:[[-0.15  1.46 -0.61  0.72 -1.25]] b:[-1.1]\n",
      "test_mode w:[[-0.15  1.46 -0.61  0.72 -1.25]] b:[-1.1]\n",
      "[Epoch:2]model loss: -16.06336369068795\n",
      "test_mode w:[[-0.15  1.46 -0.61  0.72 -1.25]] b:[-1.1]\n",
      "test_mode w:[[ 0.01  1.41 -0.5   0.9  -1.26]] b:[-1.09]\n",
      "test_mode w:[[-0.06  1.52 -0.66  0.71 -1.19]] b:[-1.08]\n",
      "test_mode w:[[-0.06  1.52 -0.66  0.71 -1.19]] b:[-1.08]\n",
      "test_mode w:[[-0.06  1.52 -0.66  0.71 -1.19]] b:[-1.08]\n",
      "[Epoch:3]model loss: -16.06336369068795\n",
      "test_mode w:[[-0.06  1.52 -0.66  0.71 -1.19]] b:[-1.08]\n",
      "test_mode w:[[ 0.1   1.47 -0.55  0.89 -1.2 ]] b:[-1.07]\n",
      "test_mode w:[[ 0.03  1.58 -0.71  0.7  -1.13]] b:[-1.06]\n",
      "test_mode w:[[ 0.03  1.58 -0.71  0.7  -1.13]] b:[-1.06]\n",
      "test_mode w:[[ 0.03  1.58 -0.71  0.7  -1.13]] b:[-1.06]\n",
      "[Epoch:4]model loss: -16.06336369068795\n",
      "test_mode w:[[ 0.03  1.58 -0.71  0.7  -1.13]] b:[-1.06]\n",
      "test_mode w:[[ 0.19  1.53 -0.6   0.88 -1.14]] b:[-1.05]\n",
      "test_mode w:[[ 0.12  1.64 -0.76  0.69 -1.07]] b:[-1.04]\n",
      "test_mode w:[[ 0.12  1.64 -0.76  0.69 -1.07]] b:[-1.04]\n",
      "test_mode w:[[ 0.01  1.45 -0.74  0.89 -1.23]] b:[-1.05]\n",
      "[Epoch:5]model loss: 0\n"
     ]
    }
   ],
   "source": [
    "class SGDOptimizer():\n",
    "\n",
    "    def __init__(self,model,lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        \n",
    "    # 一个样本的更新，实际情况下，会使用一批样本更新，这里主要是为了演示效果\n",
    "    def one_sample_optim(self,x,y):\n",
    "        model = self.model\n",
    "        if (model(x)[0] == y):\n",
    "            return \n",
    "        model.w +=self.lr * y * x\n",
    "        model.b += self.lr * y\n",
    "        \n",
    "optimizer = SGDOptimizer(test_model)\n",
    "\n",
    "test_model._init_params()\n",
    "\n",
    "print('True w:%s True b:%s' %(true_model.w,true_model.b))\n",
    "epoch = 100\n",
    "for e in range(epoch):\n",
    "    step = 0\n",
    "    for x,y in zip(data,labels):\n",
    "        step +=1\n",
    "        optimizer.one_sample_optim(x,y)    \n",
    "        if step % 10 ==9:\n",
    "            print('test_mode w:{} b:{}'.format(\n",
    "                np.around(test_model.w,decimals=2),\n",
    "                np.around(test_model.b,decimals=2)\n",
    "            ))\n",
    "    loss_data = loss(data,labels)\n",
    "    print('[Epoch:{}]model loss: {}'.format(e,loss_data))\n",
    "    if loss_data == 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多次运行可以发现，选取的初始值不同，或者选取不同的分类点，最后的解也不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法收敛性\n",
    "\n",
    "需要证明，对于**线性可分数据集**， 经过有限的迭代，可以得到一个完全正确分类的超平面以及感知模型。\n",
    "\n",
    "为了便于推导，将偏执b记入权重向量w, 即 $\\hat w = (w^T,b)^T$。 并且将输入向量扩充为$\\hat x = (x^T,1)^T$\n",
    "\n",
    "那么显然$\\hat w \\cdot \\hat b = w \\cdot x + b$\n",
    "\n",
    "证明:\n",
    "\n",
    "---\n",
    "\n",
    "由于数据集是线性可分的，那么使$||\\hat w_opt||=1$,切$w_0 = 0^n,b_0 = 0$ 切对于所有样本点，均有\n",
    "\n",
    "$y_i(w_{opt}x_i+b)>0$\n",
    "\n",
    "即\n",
    "\n",
    "$y_i(\\hat w_{opt} \\hat x)>0$\n",
    "\n",
    "假设，$\\gamma = min_i{y_i(\\hat w_{opt} \\hat x)}$\n",
    "\n",
    "那么，对于所有的i均有\n",
    "\n",
    "$y_i(\\hat w_{opt} \\hat x)>\\gamma$\n",
    "\n",
    "---\n",
    "\n",
    "如果存在误分类的点，那么就需要更新 w,b\n",
    "\n",
    "$w_k \\gets w_{k-1} + \\eta y_i x_i$\n",
    "\n",
    "\n",
    "$b_k \\gets b_{k-1} + \\eta y_i $\n",
    "\n",
    "由于\n",
    "\n",
    "$\\hat w = (w^T,b)^T$\n",
    "\n",
    "$\\hat x = (x^T,1)^T$\n",
    "\n",
    "可以得到\n",
    "\n",
    "$\\hat w_k =  w_{k-1} + \\eta y_i \\hat x_i$\n",
    "\n",
    "---\n",
    "\n",
    "需要构造 $ y_i(\\hat w_{opt} \\cdot \\hat x_i)$\n",
    "\n",
    "可以如下构造\n",
    "$\\hat w_k \\cdot \\hat w_{opt} = w_{k-1} \\cdot \\hat w_{opt} + \\eta y_i \\hat w_{opt}\\hat x_i \\ge \\hat w_{k-1} \\cdot \\hat w_{opt} + \\eta \\gamma $\n",
    "\n",
    "那么可以递推得到\n",
    "\n",
    "\n",
    "$\\hat w_k \\cdot \\hat w_{opt} \\ge \\hat w_{k-1} \\cdot \\hat w_{opt} + \\eta \\gamma \\ge \\hat w_{k-2} \\cdot \\hat w_{opt} + 2 \\eta \\gamma \\ge \\dots \\ge n \\eta \\gamma$\n",
    "\n",
    "->\n",
    "\n",
    "$\\hat w_k \\cdot \\hat w_{opt} \\ge k \\eta \\gamma$\n",
    "\n",
    "---\n",
    "\n",
    "由于$||\\hat w_opt||$已经定义为1了，还需要为$||\\hat w||$求一个上边界\n",
    "\n",
    "$||\\hat w_k ||^2 = ||\\hat w_{k-1}|| + 2 \\eta y_i \\hat w_k{-1} + \\eta ^2 ||\\hat x_i||^2  $\n",
    "\n",
    "$ \\le ||\\hat w_{k-1}|| + \\eta ^2 ||\\hat x_i||^2 $\n",
    "\n",
    "$ \\le ||\\hat w_{k-1}|| + \\eta ^2 R^2 $\n",
    "\n",
    "$ \\le ||\\hat w_{k-2}|| + 2 \\eta ^2 R^2 $\n",
    " \n",
    "$ \\le n \\eta ^2 R^2$\n",
    "\n",
    "得到\n",
    "$||\\hat w_k ||^2 \\le \\eta ^2 R^2$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "结合上面的上界和下界\n",
    "\n",
    "$\\hat w_k \\cdot \\hat w_{opt} \\ge k \\eta \\gamma$\n",
    "\n",
    "$||\\hat w_k||^2 \\le k\\eta^2R^2$\n",
    "\n",
    "可以得到\n",
    "\n",
    "$k\\eta \\gamma \\le \\hat w_k  \\hat w_{opt} \\le ||\\hat w_k||||\\hat w_opt|| \\le \\sqrt k \\eta R$\n",
    "\n",
    "由于$||w_{opt}||$为1，因此可以 得到\n",
    "\n",
    "$k^2 \\eta^2 \\le k R^2$\n",
    "\n",
    "最终得到\n",
    "\n",
    "$k\\le (\\frac R \\eta)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对偶问题\n",
    "\n",
    "对偶问题的基本思想是 将 w 和 b 表示为 $x_i$ 和 $y_i$ 的线性组合，通过求解线性组合的系数求解 w 和 b。 假设$w_0$ 和 $b_0$ 的初始值为0。那么有\n",
    "\n",
    "$w \\gets w + \\eta y_i x_i$\n",
    "\n",
    "$b \\gets  b + \\eta y_i$\n",
    "\n",
    "逐步修改 w,b 假设修改n次，w，b可以表示为\n",
    "\n",
    "$w  = \\sum_i \\alpha _i y_i x_i$\n",
    "\n",
    "$b = \\sum_i \\alpha _i y_i$\n",
    "\n",
    "其中$\\alpha_i = n_i \\eta $ n_i 表示 第i个点更新过多少次。\n",
    "\n",
    "### 感知机算法的对偶形式\n",
    "\n",
    "#### 输入：　线性可分的数据集$T = \\{(x_1,y_1),(x_2,y_2),\\dots, (x_N,y_N)\\}$, 其中　$x_i \\in R^n$, $y_i \\in \\{-1,+1\\}$,学习率$\\eta (0<\\eta<1)$\n",
    "\n",
    "#### 输出： $\\alpha,b$;感知机模型　$f(x)=sign(\\sum_{j=1}\\alpha_jy_jx_j \\cdot x +b)$,其中$\\alpha = (\\alpha_1,\\alpha_2,\\dots,\\alpha_N)^T$ \n",
    "\n",
    "\n",
    "\n",
    "#### 算法步骤：\n",
    "1. $\\alpha \\gets 0, b \\gets 0$\n",
    "2. 在训练集中选择$(x_i,y_i)$\n",
    "3. 如果　$y_i(\\sum_{j=1} \\alpha_jy_jx_j \\cdot x_i +b ) \\le 0$,\n",
    "\n",
    "    $\\alpha_i \\gets \\alpha_i + \\eta $\n",
    "    \n",
    "    $b \\gets b + \\eta y_i$\n",
    "4. 直到２.中没有错误分类的数据\n",
    "\n",
    "这里有一个技巧，由于整个计算过程中的计算量基本都在3 中的$x_j \\cdot x_i$中，并且这些计算会被反复进行，这里可以先把结果存储下来。整个矩阵叫做　Gram　矩阵。\n",
    "\n",
    "Gram 矩阵　＝　$[x_i \\cdot x_j]_{N x N}$\n",
    "\n",
    "$Gram_{ij} = x_i \\cdot x_j$\n",
    "\n",
    "\n",
    "#### 思考\n",
    "\n",
    "这里一开始阅读的时候有些不理解，有几个问题需要解释下：\n",
    "\n",
    "**１.为什么没有按照对偶问题的定义$\\alpha_i y_i$ 表示ｂ，而是直接求b。**\n",
    "\n",
    "其实我认为，按照按照对偶问题的定义，应该把ｂ表示线性组合， $b = \\sum_i \\alpha _i y_i$ 再求解。\n",
    "但是,这里的例子主要是对w进行对偶求解。 如果把b也表示为对偶形式，求解的方法和效果也是一样的。\n",
    "\n",
    "\n",
    "**２.这里的Ｎ是什么**\n",
    "\n",
    "这里的Ｎ是样本集合的大小，如果有Ｎ个样本，$\\alpha$ 就是Ｎ维的向量\n",
    "\n",
    "**3.为什么出现误分类的时 $\\alpha_i \\gets \\alpha_i + \\eta $**\n",
    "\n",
    "因为$\\alpha_i= n_i \\eta$ 如果第ｉ个样本误分类了，那么$n_i$就需要+1.\n",
    "\n",
    "$\\hat \\alpha_i$ 表示更新后的$\\alpha_i$\n",
    "\n",
    "表现在$\\alpha_i$　上就是　\n",
    "\n",
    "$\\hat \\alpha_i = (n_i + 1) \\eta$,因为迭代到目前$\\alpha_i = n_i \\eta$\n",
    "\n",
    "所以\n",
    "\n",
    "$\\hat \\alpha_i =\\alpha_i +  \\eta$\n",
    "\n",
    "**4.对偶算法简单理解。**\n",
    "\n",
    "对偶算法求得了$\\alpha$,其中 $\\alpha_i$ 表示了第i个样本在最终得到最优平面前，分类错误的次数。\n",
    "最终通过分类错误的次数来求解w和b。$w = \\sum_{i=1} \\alpha_iy_ix_i$也是由梯度下降得到的。因此它是由梯度下降法引申而来的。\n",
    "\n",
    "**5.什么是对偶问题**\n",
    "\n",
    "dual翻译为了对偶，dual的含义之一：a notion of paired concepts that mirror one another。称之为镜像问题可能更容易理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "class DualOptimizer():\n",
    "\n",
    "    def __init__(self,model,data,labels,lr = 0.1):\n",
    "        # learning rate, it is the same as eta\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.labels = labels.reshape(-1)\n",
    "        print(self.labels.size)\n",
    "        self.alpha = np.zeros((len(data)))\n",
    "        self.N = len(data)\n",
    "        self.gram_matrix = np.zeros((self.N,self.N))\n",
    "        self.generate_gram_matrix()\n",
    "        self.stop =False\n",
    "                                   \n",
    "                                   \n",
    "    # gram 矩阵\n",
    "    def generate_gram_matrix(self):\n",
    "        for i,j in itertools.product(range(self.N),range(self.N)): \n",
    "            self.gram_matrix[i][j]  = np.dot( self.data[i],self.data[j])\n",
    "            \n",
    "    \n",
    "    # 对偶形式表示的感知机\n",
    "    def dual_forward(self,i):\n",
    "        # 等同于w.x 这里使用gram的缓存\n",
    "        w_x = np.sum(self.alpha * self.labels * self.gram_matrix[i])   \n",
    "        # b 也表示为 对偶形式\n",
    "        b = np.sum(self.alpha * self.labels)\n",
    "        \n",
    "        \n",
    "        return np.sign(w_x+b)\n",
    "        \n",
    "    # 一轮更新 \n",
    "    def optim_one_epoch(self):\n",
    "        wrong_sample = 0\n",
    "        for i,d,l in zip(range(self.N),self.data,self.labels):\n",
    "            predict = self.dual_forward(i)\n",
    "            if predict * self.labels[i] <=0:\n",
    "                self.alpha[i] += self.lr\n",
    "                wrong_sample +=1\n",
    "                \n",
    "        print('[Wrong sample]: %s' % wrong_sample)\n",
    "        if wrong_sample == 0:\n",
    "            self.stop = True\n",
    "        \n",
    "        \n",
    "    def update_model_params(self):\n",
    "        w = np.sum(self.alpha * self.labels * data.T ,axis = 1)   \n",
    "        b = np.sum(self.alpha * self.labels)\n",
    "        self.model.w = w\n",
    "        self.model.b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[Epoch]: 0\n",
      "[Wrong sample]: 9\n",
      "[alpha]: [1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[Epoch]: 1\n",
      "[Wrong sample]: 6\n",
      "[alpha]: [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 2. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[Epoch]: 2\n",
      "[Wrong sample]: 3\n",
      "[alpha]: [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 2. 1. 0. 0.\n",
      " 0. 3. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[Epoch]: 3\n",
      "[Wrong sample]: 3\n",
      "[alpha]: [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 2. 1. 0. 0.\n",
      " 0. 4. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 3. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[Epoch]: 4\n",
      "[Wrong sample]: 2\n",
      "[alpha]: [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 2. 0. 0. 0. 0. 0. 2. 0. 2. 1. 0. 0.\n",
      " 0. 5. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 3. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[Epoch]: 5\n",
      "[Wrong sample]: 3\n",
      "[alpha]: [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 2. 0. 0. 0. 0. 0. 3. 0. 2. 1. 0. 0.\n",
      " 0. 6. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 4. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[Epoch]: 6\n",
      "[Wrong sample]: 0\n",
      "[alpha]: [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 2. 0. 0. 0. 0. 0. 3. 0. 2. 1. 0. 0.\n",
      " 0. 6. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 4. 0. 0. 0.\n",
      " 0. 0.]\n",
      "[model loss: 0]\n",
      "[model] w: [ -4.  53. -52.  48. -57.] b: 2.0\n"
     ]
    }
   ],
   "source": [
    "test_model._init_params()\n",
    "optim = DualOptimizer(test_model,data,labels,lr=1)\n",
    "epoch = 0 \n",
    "while not optim.stop:\n",
    "    print('[Epoch]: {}'.format(epoch,))\n",
    "    optim.optim_one_epoch()\n",
    "    print('[alpha]: {}'.format(optim.alpha))\n",
    "    epoch +=1\n",
    "    if epoch == 200:\n",
    "        break\n",
    "        \n",
    "        \n",
    "optim.update_model_params()\n",
    "loss_data = loss(data,labels)\n",
    "print('[model loss: {}]'.format(loss_data))\n",
    "print('[model] w: {} b: {}'.format(test_model.w,test_model.b))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 习题\n",
    "\n",
    "## 2.1\n",
    "\n",
    "设集合$T = \\{((0,0)-1),((0,1)1),((1,0),1),((1,1),-1) \\}$\n",
    "\n",
    "如果T是线性可分的，那么就有\n",
    "\n",
    "w.(0,0) +b <0\n",
    "w.(0,1) +b >0\n",
    "w.(1,0) +b >0\n",
    "w.(1,1) +b <0\n",
    "\n",
    "化简\n",
    "\n",
    "b<0\n",
    "w2+b>0\n",
    "w1+b>0\n",
    "w1+w2+b<0\n",
    "\n",
    "上面的式子是矛盾的，因此，亦或逻辑不是线性分类的。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.2\n",
    "上面代码已经实现\n",
    "\n",
    "## 2.3\n",
    "\n",
    "凸壳： $conv(S) = {x = \\sum_{i=1}\\lambda_ix_i|\\sum_{i=1} \\lambda_i = 1,\\lambda_i \\ge 0 } $\n",
    "\n",
    "---\n",
    "### 必要性\n",
    "\n",
    "正样本集合记为X 负样本集合记为Z\n",
    "\n",
    "如果conv(X)和conv(Z)相交，那么一定有一点，记为xz，满足下面等式\n",
    "\n",
    "$y(xz) = \\sum_{n}\\alpha_n(w\\cdot x^n + b) = \\sum_{m}\\beta_m(w\\cdot z^m+b)$\n",
    "\n",
    "$\\sum_n \\alpha _i = 1,\\alpha _i \\ge 0 $\n",
    "\n",
    "$\\sum_m \\beta _i = 1,\\beta _i \\ge 0 $\n",
    "\n",
    "线性可分，所以有：\n",
    "\n",
    "$y(x^n) = w \\cdot x^n +b >0$\n",
    "\n",
    "$y(z^m) = w \\cdot z^m +b <0$\n",
    "(1)\n",
    "\n",
    "由于alpha 和beta 都是非负数的\n",
    "\n",
    "$y(xz) = \\sum_{n}\\alpha_n(w\\cdot x^n + b) = \\sum_{m}\\beta_m(w\\cdot z^m+b)$(2)\n",
    "\n",
    "(2)和(1)冲突\n",
    "\n",
    "因此,如果线性可分，那么凸壳是不相交的\n",
    "\n",
    "\n",
    "### 充分性\n",
    "\n",
    "\n",
    "正样本集合记为X 负样本集合记为Z\n",
    "\n",
    "假设conv(X) 与 conv(Z)的距离为\n",
    "$dist(conv(X),conv(Z)) = min||x-z||,x \\in X ,z \\in Z$\n",
    "\n",
    "假设$dist(\\hat x ,\\hat z) = dist(conv(X),conv(Z)) $, 那么对于有$x \\in X$都有 $dist(x,\\hat x) \\ge dist(\\hat x ,\\hat z)$\n",
    "\n",
    "令\n",
    "\n",
    "$w = \\hat x - \\hat z$\n",
    "\n",
    "$b = -\\frac {\\hat x \\cdot \\hat x - \\hat z \\cdot \\hat z} {2}$\n",
    "\n",
    "对于所有的x都有\n",
    "\n",
    "$w.x +b = \\frac {||\\hat z - x||^2 - ||\\hat x - x||^2} 2$\n",
    "\n",
    "$=\\frac {dist(x,z)^2 - dist(x,\\hat x)^2 2$\n",
    "\n",
    "$> 0$\n",
    "\n",
    "对于所有的负样本点也有wz+b <0\n",
    "\n",
    "所有线性可分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "证明题费劲！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
