{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 统计学习方法概论\n",
    "\n",
    "第一章是对监督学习的介绍，需要掌握统计学习方法的三要素，模型选择的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 统计学习\n",
    "\n",
    "统计学习有三要修：\n",
    "- 模型 model\n",
    "- 策略 strategy \n",
    "- 算法 algorithm\n",
    "\n",
    "## 1.2 监督学习\n",
    "### 1.2.1 基本概念\n",
    "输入向量：$x_i = (x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^ T $\n",
    "\n",
    "这里的 $x_i$ 表示多个输入变量的第i个，$x^{(i)}$表示第i个特征\n",
    "\n",
    "#### 联合概率分布\n",
    "\n",
    "监督学习假设输入与输出的随机变量符合联合概率分布  $P(x,y)$\n",
    "\n",
    "并且训练数据与测试数据看做联合概率独立同分布产生的，这是监督学习的假设。\n",
    "\n",
    "#### 假设空间\n",
    "\n",
    "统计学习的目的是学习从输入到输出的映射，这个映射属于的空间叫做假设空间(hypothesis space)\n",
    "\n",
    "## 1.3 统计学习三要素\n",
    "\n",
    "### 1.3.1 模型\n",
    "模型就是要学习的条件分布或者决策函数\n",
    "\n",
    "#### 决策模型\n",
    "假设空间用$F$表示，假设空间可以定位决策函数的集合 \n",
    "\n",
    "$F={f|Y=f(x)}$\n",
    "\n",
    "一般而言，$F$是由参数$\\theta$向量决定的函数族 \n",
    "\n",
    "$F={f|Y=f_{\\theta}(x),\\theta \\in R^n }$\n",
    "\n",
    "参数$\\theta$属于n维的欧氏空间$R^n$，参数参数空间\n",
    "\n",
    "#### 概率模型\n",
    "\n",
    "假设空间也可以用概率表示\n",
    "\n",
    "$F={P|P(Y|X)}$\n",
    "\n",
    "一般而言，$F$是由参数$\\theta$向量决定的函数族 \n",
    "\n",
    "$F={f|P=P_{\\theta}(x),\\theta \\in R^n }$\n",
    "\n",
    "参数$\\theta$属于n维的欧氏空间$R^n$，也参数参数空间\n",
    "\n",
    "### 1.3.2 策略\n",
    "\n",
    "#### 损失函数\n",
    "\n",
    "**损失函数度量模型的一次预测好坏**\n",
    "\n",
    "常见的损失函数\n",
    "\n",
    "##### 0-1 loss function\n",
    "\n",
    "$L(Y,f(x)) = \\left\\{  \\begin{array}{ccc}\n",
    "            1, Y != f(x)\\\\ \n",
    "           0, Y = f(x)\\end{array} \\right. $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-1 loss between\n",
      "tensor([3., 9., 1., 0., 9., 7., 1., 5., 3., 7., 2., 2., 6., 0., 6., 4., 8., 5.,\n",
      "        6., 9.])\n",
      "tensor([1., 6., 6., 5., 2., 5., 0., 8., 5., 1., 2., 3., 6., 7., 3., 6., 1., 4.,\n",
      "        7., 5.])is\n",
      "0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dummy_f_x = torch.floor(torch.rand((20,1))*10).view(20)\n",
    "dummy_y = torch.floor(torch.rand((20,1))*10).view(20)\n",
    "loss = (dummy_y== dummy_f_x ).float().mean()\n",
    "print('0-1 loss between\\n{}\\n{}is\\n{}'.format(dummy_y,dummy_f_x,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quadratic loss function 平方损失函数\n",
    "\n",
    "$L(Y,f(X))=(Y-f(x))^2$\n",
    "#### abssole loss function 绝对损失函数\n",
    "\n",
    "$L(Y,f(X))=|Y-f(x)|$\n",
    "    \n",
    "##### logarithmic loss function 对数损失函数\n",
    "\n",
    "$L(Y,f(X))=-logP(Y|X)$\n",
    "\n",
    "#### 1.3.2.1 风险函数（risk function） 期望损失(expected loss)\n",
    "\n",
    "模型的输入X和输出Y是随机变量，并且遵守P(X,Y)的联合分布，所以损失函数的期望为：\n",
    "\n",
    "$R_{exp}(f) = E_p[L(Y,f(x))] = \\int_{x \\times y}L(y,f(x))P(x,y)dxdy$\n",
    "\n",
    "但是由于我们无法知道$P(X,Y)$这个联合分布，\n",
    "\n",
    "通常会使用如下的经验风险:\n",
    "\n",
    "$R_{emp}(f)=\\frac 1 N \\sum_{t=1}^N L(y_i,f(x_i))$\n",
    "\n",
    "根据大数定理，当样本容量N趋于无穷的时候，$R_{emp}$趋于$R_{exp}$\n",
    "\n",
    "**由于训练样本数量有限，监督学习有两个基本策略，经验风险最小和结构风险最小**\n",
    "\n",
    "#### 1.3.2.2 风险最小化与结构风险最小化\n",
    "\n",
    "经验风险最小化(empirical risk minimization,ERM)策略可以表示为\n",
    "\n",
    "$\\min_{f \\in F} \\frac 1 N \\sum L(yi,f(x_i))$\n",
    "\n",
    "极大似然估计就是采用的经验最小化的策略。\n",
    "\n",
    "如果样本不多，很容易产生over-fitting现象，那么就引入了(structural risk minimization,SRM)结构风险最小化。**结构风险最小化等价于正则化**结构风险最小化的定义为：\n",
    "\n",
    "$R_{erm}(f) = \\frac 1 N L(y_i,f(x_i)) + \\lambda J(f)$\n",
    "\n",
    "$J(f)$为模型的复杂度，f越复杂，J(f)越大，贝叶斯估计的最大后验估计（maxinum posterior probability estimation,MAP）就是结构风险最小化的例子.\n",
    "\n",
    "求解结构风险最小化的模型就转化为：\n",
    "$\\min_{f \\in F} \\frac 1 N \\sum_{i=1}^N L(y_i,f(x_i))+\\lambda J(f)$\n",
    "\n",
    "### 1.3.3 算法\n",
    "\n",
    "算法是指学习模型的具体方法。统计学习问题归结为最优化问题，如果有明显的解析解，那么优化问题比较简单。\n",
    "\n",
    "否则就要利用一个算法，高效的找到最优解。\n",
    "\n",
    "以上三个要素决定了统计学习的方法，这三个要素确定了，统计学习的方法也就确定了。\n",
    "\n",
    "## 1.4 模型评估与模型选择\n",
    "\n",
    "### 1.4.1 训练误差和测试误差\n",
    "\n",
    "- 测试误差更好的模型更好\n",
    "\n",
    "### 1.4.2 过拟合与模型选择\n",
    "\n",
    "复杂的模型容容易过拟合，我们常用**交叉验证**和**正则化**来避免这种情况。\n",
    "\n",
    "### 1.5.1 正则化\n",
    "\n",
    "正则化符合奥卡姆剃刀原则，举例来说，如果损失函数是平方损失，正则化项是参数向量$L_2$范数则：\n",
    "\n",
    "$L(w) = \\frac 1 N \\sum_{i=1}^N(f(x_i;w)-y_i)^2+\\frac \\lambda 2 +||w||^2$\n",
    "\n",
    "### 1.5.2 交叉验证\n",
    "- 简单交叉验证\n",
    "- S折交叉验证\n",
    "- 留一交叉验证\n",
    "\n",
    "## 1.6 泛化能力\n",
    "\n",
    "定义：如果学到的模型是f,那么这个模型对未知数的预测的误差就叫做泛化误差，实际上，它就是模型的期望风险。\n",
    "\n",
    "$R_{exp}(f) = E_p[L(Y,f(x))] = \\int_{x \\times y}L(y,f(x))P(x,y)dxdy$\n",
    "\n",
    "### 1.6.2 泛化误差上界\n",
    "\n",
    "- 定理1.1 对于二分类问题，如果假设空间是有限个函数集合$F = {f_1,f_2...f_d}$ 对于任意函数$f \\in F$ 至少以概率 $1-\\delta$ 使下面不等式成立：\n",
    "\n",
    "$R(f)<\\hat R(f) + \\epsilon (d,N,\\delta)$\n",
    "\n",
    "其中：\n",
    "\n",
    "$\\epsilon (d,n, \\delta) = \\sqrt {\\frac 1 2N (log d + log \\frac 1 \\delta)}$\n",
    "\n",
    "可以看出，训练误差越小，泛化误差也越小。N越大，泛化误差越小；当N无穷大的时候，他是 $\\sqrt {log d}$阶的函数，d越大，泛化误差越大。\n",
    "\n",
    "## 1.7 生成模型和判别模型\n",
    "\n",
    "### 生成方法\n",
    "\n",
    "由数据学习联合概率分布 $P(X,Y)$ 然后求条件概率分布 $P(Y|X)$ 作为预测的模型，即：\n",
    "\n",
    "$P(Y|X) = \\frac {P(X,Y)} {P(X)}$\n",
    "\n",
    "这种方式叫做生成方法的原因的，模型表示了对于给定 X 产生 Y 的生成关系。\n",
    "\n",
    "典型的生成模型：\n",
    "- 朴素贝叶斯\n",
    "- 隐马尔科夫\n",
    "\n",
    "优点：\n",
    "- 生成方法可以还原出联合概率分布$P(X,Y)$;\n",
    "- 学习收敛速度更快；\n",
    "- 存在隐变量也可以用生成方法学习。\n",
    " \n",
    "### 判别方法\n",
    "\n",
    "判别方法直接学习 $f(x)$ 或者是 $P(Y|X)$ 作为预测的模型。判别模型关系的对于给定的X，应该预测什么样的输出。\n",
    "\n",
    "典型的判别模型：\n",
    "- k近邻法\n",
    "- 感知机\n",
    "- 决策树\n",
    "- 逻辑斯谛回归模型\n",
    "- 最大熵\n",
    "- 支持向量机\n",
    "- 提升方法\n",
    "- 条件随机场\n",
    "\n",
    "## 1.8 统计学习分类\n",
    "- 分类问题，输出是一个类别\n",
    "- 标注问题，输入是一个序列，输出也是一个序列\n",
    "- 回归问题，输出是一个连续值\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 习题\n",
    "\n",
    "## 1.1 \n",
    "\n",
    "\n",
    "### 最大似然估计：\n",
    "\n",
    "- 模型\n",
    "\n",
    "伯努利分布：\n",
    "\n",
    "$P(X = K) = C_n^k\\theta ^k(1-\\theta )^{n-k}$\n",
    "\n",
    "最终的问题要求 k = 1 时候的概率\n",
    "\n",
    "求 x = 1 时的 y 值 \n",
    "其中 x 表示出现的次数,y 表示出现 x 次数的概率\n",
    "\n",
    "那么有：\n",
    "\n",
    "$y = f(x) = P(X = x) = C_n^xp^x(1-p)^{n-x}$\n",
    "\n",
    "这个问题可以看成近似一个回归问题\n",
    "\n",
    "- 策略\n",
    "\n",
    "最大似然估计选取模型的策略为，使得模型产生当前情况的概率最大。即：\n",
    "\n",
    "$L(\\theta) = C_n^k \\theta ^k(1-\\theta)^{n-k} $\n",
    "\n",
    "$\\theta = arg max_\\theta L(\\theta) = k\\theta + n-k(1-\\theta) =\\frac k n $\n",
    "\n",
    "- 算法\n",
    "\n",
    "要求上面问题的极值，需要求微分为零的点：\n",
    "\n",
    "$ \\frac d d\\theta (\\theta ^ k(1-\\theta) ^{n-k}) = 0$\n",
    "\n",
    "化简后得到\n",
    "\n",
    "$\\theta ^ k(1-\\theta)^{n-k}[k(1-\\theta) - (n-k)\\theta )$\n",
    "\n",
    "有三个解，$\\theta = 0, \\theta =1, \\theta = k/n$\n",
    "\n",
    "\n",
    "其中使 $L(\\theta)最大的解为 k/n $\n",
    "\n",
    "因此模型求解得模型为\n",
    "\n",
    "$P(X = K) = C_n^kp^k(1-p)^{n-k}$\n",
    "\n",
    "那么k = 1 的概率为：\n",
    "\n",
    "$P(X = K) = \\frac {p(1-p)^{n-1}} n$\n",
    "\n",
    "### 贝叶斯估计：\n",
    "\n",
    "- 模型\n",
    "\n",
    "模型不变，还是\n",
    "\n",
    "$y = f(x) = P(X = x) = C_n^xp^x(1-p)^{n-x}$\n",
    "\n",
    "- 策略\n",
    "\n",
    "贝叶斯流派把$\\theta$视为一个随机分布\n",
    "\n",
    "在这个问题中$\\pi(\\theta) = 1/1$,其中$\\pi(\\theta)$表示theta的先验分布。这里只直到 $\\theta$ 属于 0,1 之间，因此把它看做为均匀分布。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1.2\n",
    "假设x是离散变量\n",
    "\n",
    "- 模型\n",
    "\n",
    "\n",
    "$F={P|P_{\\theta}(Y|X)}$\n",
    "\n",
    "- 损失函数\n",
    "\n",
    "$L(T,P(Y|X)) = -logP(Y|X)$\n",
    "\n",
    "经验风险最小化：\n",
    "\n",
    "$min_L = min_\\theta \\sum log P(Y|X)$\n",
    "\n",
    "最大似然估计：\n",
    "\n",
    "$Like(\\theta|x_1,x_2,...x_n) = \\prod_{i=1}^nf(x_i|\\theta)$\n",
    "\n",
    "要求 最大的Like,可以给上面的式子求对数\n",
    "\n",
    "$\\sum_{i=1}^n log(f(x_i|\\theta)) = \\sum_{i=1}^n log P_\\theta(Y|X)$\n",
    "\n",
    "就等于最小化\n",
    "\n",
    "$ -\\sum_{i=1}^n log P_\\theta(Y|X)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10 总结\n",
    "## 对数损失函数如何计算\n",
    "$L(Y,P(Y|X)) = -logP(Y|X) = - \\frac 1 N \\sum_{i=1} \\sum_{j=1} y_{ij} log(P_{ij})$\n",
    "\n",
    "其中 N为输入的样本数量，M为可能的类别，$y_{ij}表示$ 类别j是否是x_i的真实类别,p_{ij} 表示输入实例x_i 属于类别j 的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use self-defined logloss() in binary classification, the result is 0.1738073366910675\n",
      "Use log_loss() in scikit-learn, the result is 0.1738073366910675 \n"
     ]
    }
   ],
   "source": [
    "def logloss(y_true, y_pred, eps=1e-15):\n",
    "    import numpy as np\n",
    "\n",
    "    # Prepare numpy array data\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    assert (len(y_true) and len(y_true) == len(y_pred))\n",
    "\n",
    "    # Clip y_pred between eps and 1-eps\n",
    "    p = np.clip(y_pred, eps, 1-eps)\n",
    "    loss = np.sum(- y_true * np.log(p) - (1 - y_true) * np.log(1-p))\n",
    "\n",
    "    return loss / len(y_true)\n",
    "\n",
    "\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [0.1, 0.2, 0.7, 0.99]\n",
    "\n",
    "print (\"Use self-defined logloss() in binary classification, the result is {}\".format(logloss(y_true, y_pred)))\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print (\"Use log_loss() in scikit-learn, the result is {} \".format(log_loss(y_true, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型选择\n",
    "交叉验证和正则化，他们不是用来做训练的，不要和模型训练混淆\n",
    "\n",
    "## 监督学习的概括\n",
    "\n",
    "1. 数据独立同分布\n",
    "2. 模型属于一个假设空间\n",
    "3. 使用某一个评价标准，选择一个最优的模型\n",
    "4. 使它在已知数据集合和未知数据集合上有最准确的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
